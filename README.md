# openwebui-non-docker
A lightweight local-first web-based UI for running LLMs via Ollama. This setup runs completely outside Docker and is optimized for high-memory servers.
